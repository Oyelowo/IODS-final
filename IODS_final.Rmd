---
title: "IODS_final"
author: "Oyedayo Oyelowo"
date: "16 December 2017. Email: oyelowo.oyedayo@helsinki.fi"
Email: "oyelowo.oyedayo@helsinki.fi"
output: html_document
---

#ABSTRACT.

This project looks into the factors affecting academic performances of students, absences and also alcohol consumption pattern. Multiple Correspondence Analysis, regression models(GLM, GAM and GBM), and Linear Discriminant analysis were used to explore the data. The result shows that female students are more successful in their studies than male students because of several factors. They get more supports for studies, are more ambitious to further to higher education, and less absent, have less high alcohol consumption, than thier male counterparts.

Mothers' education levels, to a considerable extent, also affect students' performances and based on the MCA, female students are closer to their mother, which might be another underlying factor for their better success ahead of male students in their studies.

Different measures were adopted to validate the models. Of all the techniques adopted, the regression models seemed to be the weakest and might require more observations and/or predictors. The prediction for the binomial variabl if high alcohol consumption was however, fairly good, after being evaluated by AUC, with average value of about 0.7 for the various modes. The precision(o.75) and accuracy(0.79) were quite good too but performed better when predicting FALSE.

By and large, the result shows that female students perform better and describes the factors which play roles in their success.


###Loading the data set and packages
```{r, message=FALSE, warning=FALSE}
#clear environment
rm(list=ls())

#read data
fpath<- "C:/Users/oyeda/Desktop/OPEN_DATA_SCIENCE/IODS-final/data/"
alco_data <- read.table(paste0(fpath, "alcohol_student.csv"), sep=",", header=T)
alco_data<- alco_data[,-1]

#necessary packages
#install.packages("dismo")
library(dplyr)
library(ggplot2)
library(corrplot)
library(GGally)
library(tidyr)
library(tidyverse)
library(gbm)
library(dismo)
library(caTools)
library(mgcv)
library(MASS)
library(FactoMineR)
```


#AIMS AND OBJECTIVES 
My aim is to demonstrate the use of various statistical techniques in getting insight in to
the dataset, to understand the patterns alcohol consumption consumptions and absences amongst students in two schools. Further, I seek to explore the distribution of grades amongst male and female students; and factors that might be affecting this.
Firstly, I will use the basic desctiptive statitstics to understand the distribution, correlation and dependencies(cor, barplot, histogram, biplot etc)
I will also be using the regression models(_**Generalized Linear Model(GLM), Generalised Additive Model(GAM) and Generalised Boosted Model(GBM))**_. Furthermore, I will test my models using 70:30 data split.

I will also explore Linear Discriminant Analysis(LDA) and  Multiple Correspondence Analysis(MCA), to classify and categorise the data.


#RESEARCH QUESTIONS
-  What are the factors considerably affecting students' grades?

-  What are the factors causing absences amongst students?

-  What are the factors significantly causing high alcohol consumption amongst students.

## Hypotheses:
-  High alcohol consumption has no effect on students' performances.

- students' grades are not affected by absences.

-  Mothers' level of education does not affect their kids' academic performances.

- Male and Female students get same level of support.


#DATA EXPLORATION AND DESCRIPTION.
The data includes 35 variables and 382 observations which describe the performances of secondary school students in two distinct subjects: Mathematics and Portuguese. It also contains demographic, social and other information about schooling. Other information related to the original data can be found from [this source](https://archive.ics.uci.edu/ml/datasets/Student+Performance). 

After downloading the data, it was wrangled by combining the two different datasets related to Performances in Mathematics and Portuguese. Thereafter, Weekend and weekdays alcohol consumptions were combined by finding their average. Afterwards, a thresold of 0.5 was selected for high alcohol consumption and a boolean variable("high_use") was created from that. **_The link to my DATA WRANGLING is_**[here.](https://github.com/Oyelowo/IODS-final/blob/master/data_wrangling_alc.R)

The wrangled data can also be found [here.](https://github.com/Oyelowo/IODS-final/blob/master/data/alcohol_student.csv)



The response/target variables of interest are: **_grades(G3), high alcohol consumption(high_use), alcohol consumption(alc_use) and absences_**. G3 is the final year grade of the students. Other information related to the description of the variables can be found from the source given above.




```{r, message=FALSE, warning=FALSE}
categ = apply(alco_data, 2, function(x) nlevels(as.factor(x)))
#categ
dim(alco_data)
str(alco_data)
summary(alco_data)
#glimpse(alco_data)

# ####Grade:
#bar plot of the variables
gather(alco_data) %>% ggplot(aes(value)) + facet_wrap("key", scales = "free") + geom_bar() 


```

  The above shows the distribution of the chosen predictors and also scale of alcohol consumption. The ages of the students are mainly within 15-19 with few students aged 20 and 22. Generally, there are relatively few chronic alcohol consumers amongst the students. The students seem to have quite enough free time. The respondents include 198 female students and 184 male students.
  
```{r, message=FALSE, warning=FALSE}
hyp<- alco_data[,c("age", "sex",  "absences","freetime","alc_use")]
#show the overview of the predictors and response variable(non-binomial)
plot_hyp <- ggpairs(hyp, mapping = aes(col=sex, alpha=0.3), lower = list(combo = wrap("facethist", bins = 20)))
plot_hyp   #draw the plot
```  
  
  Here, we can see that the female students seem to be generally quite older and there are both male and female students that are much older than the general sample of the students. The absences are not so high but some chronic absentees amongst the students(both male and female). Most of the students are quite free but a few of the students are very busy. Largely, all the predictors show no correlation, hence, the issue of multicolinearity is not a problem here. However, these predictors still show very weak correlation with alcohol consumption. Nevertheless, it should be recalled that correlation is not causation. Hence, I will dig further to understand the effects of these predictors on alcohol consumption amongst students and see if they are significant.
  
  
  Crosstabs
The below are crosstabs that compare the predictors with alcohol consumption

```{r, message=FALSE, warning=FALSE}
#using the binomial response(high_use)
#it is also possible to use simple crosstabs e.g
#xtabs(high_use~age, data = alc)
#but below is a more comprehensive crosstab.
#Crosstabs
summarise(group_by(alco_data, age,high_use), count=n(), mean_grade=mean(G3))

#an alternative and preferrable way of doing the above
#alc %>% group_by(age, high_use) %>% summarise(count=n(), mean_grade =  mean(G3))

summarise(group_by(alco_data, sex,high_use), count=n(), mean_grade=mean(G3))


summarise(group_by(alco_data, absences,high_use), count=n(), mean_grade=mean(G3))
```
  
  
  The age distribution by sex
```{r, message=FALSE, warning=FALSE}  
# initialize a plot of 'age'
sex_age <- ggplot(data = alco_data, aes(x=age, col=sex))

# draw a bar plot of age by sex
sex_age + geom_bar() + facet_wrap("sex") 
```


**High alcohol consumption by sex**
```{r, message=FALSE, warning=FALSE}
# initialize a plot of 'high_use'
hu_sex <- ggplot(data = alco_data, aes(x=sex, col=sex))

# draw a bar plot of high_use by sex
hu_sex + geom_bar() + facet_wrap("high_use")  
```

Here, we can see that there are more female who do are not high consumers of alcohol compared to men. Men consume more alchols than their female counterparts



**High use vs absences**
```{r, message=FALSE, warning=FALSE}
# initialize a plot of 'high_use'
hu_ab <- ggplot(data = alco_data, aes(x=absences, col=sex))

# draw a bar plot of 'high_use' by freetime
hu_ab + geom_bar() + facet_wrap("high_use")
```


**High use vs age**
```{r, message=FALSE, warning=FALSE}
# initialize a plot of 'high_use'
hu_ag <- ggplot(data = alco_data, aes(x=age, col=sex))

# draw a bar plot of 'high_use' by freetime
hu_ag + geom_bar() + facet_wrap("high_use")
```


**High use vs romantic**
```{r, message=FALSE, warning=FALSE}
# initialize a plot of 'high_use'
hu_rom <- ggplot(data = alco_data, aes(x=romantic, col=sex))

# draw a bar plot of 'high_use' by freetime
hu_rom + geom_bar() + facet_wrap("high_use")
```

For students in romantic relationships, they generally have lower number of them that are high consumers of alcohol. This is same with those in no romatic relationship.



**BOXPLOTS**
**Below are boxplots to give clearer pictures, as explained earlier. ##Relationship of alcohol consumption with absences**

```{r, message=FALSE, warning=FALSE}
# initialise a plot of high_use and absences
h_ab<- ggplot(alco_data, aes(x=high_use, y=absences,col=sex))

# define the plot as a boxplot and draw it
h_ab + geom_boxplot() + ggtitle("Student absences vs alcohol consumption")
```

There are a few chronic absentees amongst male and female students, but generally, most of the students have low absences.


**Relationship of alcohol consumption with age**
```{r, message=FALSE, warning=FALSE}
# initialise a plot of high_use and age
h_ag<- ggplot(alco_data, aes(x=high_use, y=age,col=sex))

# define the plot as a boxplot and draw it
h_ag + geom_boxplot() + ggtitle("Students' age vs alcohol consumption")
```


**Relationship of alcohol consumption with sex**

```{r, message=FALSE, warning=FALSE}
# initialise a plot of high_use and sex
alc_sex<- ggplot(alco_data, aes(y=alc_use, x=sex,col=sex))

# define the plot as a boxplot and draw it
alc_sex + geom_boxplot() + ggtitle("Student's alcohol consumption by sex")
```

There are a few female students with quite high alcohol consumption but the alcohol consumption levels do not vary as much as they do amongst the male students. Generally, male students tend to consume more alcohol.



#METHODOLOGY.
To understand factors that could affect students' performances, alcohol consumption pattern and absences, I will be utilising three different approaches. The first is the Multiple Correspondence Analysis(MCA) which is used for categorical variables. I will firstly explore the already categorised variables and further categorise more continuous variables for MCA. This is useful to understand the associations amongst the variables.

The second approach involves the use of various regression models: Generalised Linear Model(GLM), Generalised Additive Model(GAM) and General Boosted Model(GBM). These will be used to further understand the causations. Furthermore, two GBM packages -"gbm" and "dismo"- will be used to compare their performances. In selection of the predictors for GLM, stepwise regression, t-test from model summary and anova(Chi square) test will be performed to eliminate redundancies. In GAM, variables will be selected in similar manner, and a smoothing spline function be used with appropriate degree of freedom.

Also, the models of the grades(G3) and absences, will be evaluated, by using correlation, Mean Absolute error and Root Mean Square Error(RMSE). However, the modells of binomial response variable - high alcohol consumption(high_use) -  will be evaluated by adopting the Receiver operating characteristic(ROC) Area Under Curve(AUC). Confusion/Classification matrix will also be used to evaluate the precision, accuracy, sensitivity and specificity. The odds ratio will also be looked into.


In the course of the evaluation of the regression models, the data will be divided into 70% calibration or training data and 30% evaluation or testing data. The data is also sampled by replacement and boostrapped ten times.

Lastly, Linear Discriminant Analysis will be performed on categorised grades(G3) and absences. it will also be evaluated by a classification matrix.



#RESULTS.
  To avoid, confusion, I will copy the data into a new data frame for MCA. Firstly, I will be extracting the categorical variables, as factors.  

##MULTIPLE CORRESPONDENCE ANALYSIS(MCA)
```{r, message=FALSE, warning=FALSE}
#To avoid, confusion, I will copy the data into a new data frame for MCA
data_mca <- alco_data

#Firstly, I will be extracting the categorical variables, as factors
#now, filter them out for the MCA
data_mca_fac1<-Filter(is.factor, data_mca)

high_use<-factor(alco_data[,"high_use"])
data_mca_fac1 <- cbind.data.frame(data_mca_fac1, high_use)

```

#MCA plot
```{r, message=FALSE, warning=FALSE}
#par(mfrow=c(1,2))
#now, perform the MCA on the catergorial/qualitative variables
mca_alco1 <- MCA(data_mca_fac1, graph = T)

#plot it
plot(mca_alco1, invisible=c("ind"), habillage="quali")
#par(mfrow=c(1,1))


```

**BETTER VISUALISATION FOR MCA**
```{r, message=FALSE, warning=FALSE}
# Getting a better biplot for MCA using ggplot
## number of categories per variable
categ = apply(data_mca_fac1, 2, function(x) nlevels(as.factor(x)))
categ

# data frame with variable coordinates
mca_vars_df = data.frame(mca_alco1$var$coord, Variable = rep(names(categ), categ))
#mca_vars_df


# data frame with observation coordinates
mca_obs_df = data.frame(mca_alco1$ind$coord)


# MCA plot of observations and categories
ggplot(data = mca_obs_df, aes(x = Dim.1, y = Dim.2)) +
  geom_hline(yintercept = 0, colour = "gray70") +
  geom_vline(xintercept = 0, colour = "gray70") +
  geom_point(colour = "gray50", alpha = 0.0) +
  geom_density2d(colour = "gray80") +
  geom_text(data = mca_vars_df, 
            aes(x = Dim.1, y = Dim.2, 
                label = rownames(mca_vars_df), colour = Variable)) +
  ggtitle("MCA plot of variables using R package FactoMineR") +
  scale_colour_discrete(name = "Variable")
```

  Here, we can see that female students generally attend school
because of her reputation and get more school support.
They also have family support and are able to attend paid extra
classes. Most of their father's job are in the health sector.
High alcohol consumption is more rampant amongst female students
compared to their male counterparts with high alcohol use.
male students also attend the school based on course preference
and other reasons. By and large, they do not attend paid extra classes compared to the female students and also do not get family and school support like the frmale students.
Lastly, they mosly have no intention of pursuing higher education. This is explored further by categorising more continuous variables such as grades, absences, to allow for comparison with other variables.



**CATEGORISE MORE VARIABLES**
 
 Next, I will be categorising **_grade(G3), absences, Mother's education(Medu), father's education_**
```{r, message=FALSE, warning=FALSE}
#Next, I will be categorising grade(G3), absences, Mother's education(Medu),
#father's education
data_mca2<- alco_data

#now, convert mother and father's education level into something more understandable.
#data_mca$Medu<- factor(data_mca$Medu)
data_mca2$Medu<- factor(data_mca2$Medu, labels=c("no", "pr","5-9th", "sec", "hiE"))
data_mca2$Fedu<- factor(data_mca2$Medu, labels=c("no", "pr","5-9th", "sec", "hiE"))

#Now, let's categorise grades according to quartiles
bins_abs<- quantile(data_mca2$absences)
data_mca2$absences<-cut(data_mca2$absences, breaks=bins_abs, include.lowest=T,
                       labels=c("vlow", "low", "high", "vhigh"))

#same to grade(G3)
bins_gra<- quantile(data_mca2$G3)
data_mca2$G3<-cut(data_mca$G3, breaks=bins_gra, include.lowest=T,
                 labels=c("vlow", "low", "high", "vhigh"))

#Getting the columns that are factors
#since MCA is for categorical variables, I will be filtering the categorical
#variables/factors and also categorise some of the variables of interest
#such as absences, grade(G3). I will also Categorise other variables of interest that are in integer forms.

#let's first see the variables already in factor format
names(Filter(is.factor, data_mca2))

#now, filter them out for the MCA
data_mca_fac2_<-Filter(is.factor, data_mca2)

#include the high alcohol use column
high_use<-factor(alco_data[,"high_use"])

#join with the dataframe with categorical varianles
data_mca_fac2_<-cbind.data.frame(data_mca_fac2_, high_use)
str(data_mca_fac2_)

#The above can also be done with dplyr which has been loaded already too
#data_mca %>% Filter(f = is.factor)

#names of the variables again ?
names(data_mca_fac2_) 
#Alternative for finding the column name that are factors
# names(data_)[ sapply(data_reg, is.factor) ]
# #or
# data_reg %>% Filter(f = is.factor) %>% names
# or
# data_reg %>% summarise_each(funs(is.factor)) %>% unlist %>% .[.] %>% names


#I will select few of the variables for clarity sake and include the sex too
data_mca_fac2<-data_mca_fac2_[,c("sex",names(data_mca_fac2_[,(10:ncol(data_mca_fac2_))]))]
    
#now, perform the MCA on the catergorial/qualitative variables
mca_alco2 <- MCA(data_mca_fac2, graph = T)

#plot it
plot(mca_alco2, invisible=c("ind"), habillage="quali")



# Getting a better biplot for MCA using ggplot
## number of categories per variable
categ2 = apply(data_mca_fac2, 2, function(x) nlevels(as.factor(x)))
categ2

# data frame with variable coordinates
mca_vars_df2 = data.frame(mca_alco2$var$coord, Variable = rep(names(categ2), categ2))
#mca_vars_df2


# data frame with observation coordinates
mca_obs_df2 = data.frame(mca_alco2$ind$coord)


# MCA plot of observations and categories
ggplot(data = mca_obs_df2, aes(x = Dim.1, y = Dim.2)) +
  geom_hline(yintercept = 0, colour = "gray70") +
  geom_vline(xintercept = 0, colour = "gray70") +
  geom_point(colour = "gray50", alpha = 0.1) +
  geom_density2d(colour = "gray80") +
  geom_text(data = mca_vars_df2, 
            aes(x = Dim.1, y = Dim.2, 
                label = rownames(mca_vars_df2), colour = Variable)) +
  ggtitle("MCA plot of variables") +
  scale_colour_discrete(name = "Variable")

```


Here, we can see the categorisation and association
In the NW quadrant, it shows that those that pay for for extra
classes in portugese and math, have family support and are mostly
female students. They also get extra education support and have
ambition for higher education.


  In the NE quadrant, high alcohol use seem to be associated with
guardian other than mother or father. Those with high
alcohol use also seem to not have ambition for higher education.
They also perform poorly in studies(low grade). and mostly choose
school because it is closer to home amongst other reasons, other
than reputation and course preference. They also do not seem to
participate in extracurricular activities and are mostly
in romantic relationship. They also high absences.

  In the SW quadrant, it can be seen that those that have low absence
perform very well in studies do not take excessive alcohol. They
are also engaged in extracurricular activities and have their mother
as their guardian. They also motivated to attend the school
because of shool's reputation.

  In the SE quadrant, Male students seem to be more moderately absent and do not attend paid extra classes in portuguese and math. They mostly attend the school because of the course preference. They generally also have no internet access. They also mostly do not get school's support. It also looks like their father are their guardian.

  This will be further explored with family of regression models
**(GLM, GAM, GBM)**



##REGRESSION MODELS(GLM, GAM, GBM)

 GLM, GAM, GBM are quite similar but are respectively increasingly more detailed in delivering their predictions. They are all capable of handling not only gaussian distribution but also binomial and Poisson without having to transform the response variable via the identity function which uses logit and log for binomial and Poisson distributions respectively. 
 
 However, GAM has the advantage over GLM of being able to present some response curves from the models more readily. It offers more flexibility. In similar vein, GBM offers even more flexibility in dealing with the explanatory variables and their interactions. By recursive binary splitting also, it is able to produce model with improved predictive performance and less error. This is done with the number of internal iterations in it. It also presents the relative importance of the predictors. 

  Before I proceed, there is need to determine the error distribution of the response variables here. As in many realistic cases, we do not have normal distribution(i.e gaussian), we could also have poisson distribution for count data and last is the binomial distribution
(e.g Yes/No, Presence/Absence). That said, one of the assumptions of poisson distribution is
that variance is greater than mean. Therefore, I will be creating
a function to test this assumption and decide the kind  of distribution.
You can see more [here](http://datavoreconsulting.com/programming-tips/count-data-glms-choosing-poisson-negative-binomial-zero-inflated-poisson/)



**Create function to test variance>mean assumption of poisson distribution**
```{r, message=FALSE, warning=FALSE}
#Create function to test variance>mean assumption of poisson distribution
test.var.mn<- function(x){
  if(var(x)>mean(x)){
    print("VALID:The variance>mean assumption of poisson distribution is valid")
  }
  else{
    print("INVALID:The variance>mean assumption of poisson distribution is invalid")
  }
}

```



**TEST THE ASSUMPTION OF POISSON DISTRIBUTION ON AGE AND ABSENCES**
```{r, message=FALSE, warning=FALSE}
#see if Grade(G3) meets the assumption
test.var.mn(alco_data$G3)


#next, see if "absences" meets the assumption
test.var.mn(alco_data$absences)

```

It shows here that it is invalid for grade(G3) but valid for "absences" variable. Next, therefore, I will be exploring various regression models, with grade(G3) as gaussian, Absences as poisson and high alcohol use(High_use) as binomial.


  As done earlier, the data will be copied into a new dataframe for all the regression analysis, to avoid confusion and alteration of the original data. 
  
  For GLM, predictors will be selected, by firstly using stepwise regression to eliminate the redundant variables. Afterwards, I will be employing the significance test from the model summary, anova test(Chi Square), and AIC values. I also checked if any of the predictor is curvillinear(i.e has higher order polynomial).


```{r, message=FALSE, warning=FALSE, include=FALSE}
#As done earlier, the data will be copied into a new dataframe for 
#all the regression analysis, to avoid confusion and alteration of 
#the original data
data_reg <- alco_data

#for GLM, predictors were selected, by employing the significance test from the model
#summary, anova test(Chi Square), AIC values and stepwise regression. I also checked 
#if any of the predictor is curvillinear(i.e has higher order polynomial)


grade_glm<-glm(G3~ sex + age+address+Medu+Fedu+
                 Pstatus+ traveltime+studytime+famsup+activities+higher+
                 internet+famrel+romantic+freetime+goout+ alc_use+ absences
               ,data=data_reg,family ="gaussian")

#First, I used the stepwise regression to eliminate the redundant variables.
stepAIC(grade_glm, direction = "both")

#I got this: glm(G3~ address + Medu + studytime + higher + romantic 
#goout,data=data_reg,family ="gaussian")


#Thereafter, anova Chi square test and the T.test from the model summary
#were used to further eliminate the insiginificant variables at level 0.05
#After this, "romantic" was slightly below the level and hence, removed
grade_glm<-glm(G3~ address + Medu  + studytime + higher + 
                 goout,data=data_reg,family ="gaussian")

#Anova test
anova(grade_glm, test="Chisq")

```

Anova Chi square test and the T.test from the model summary were used to further eliminate the insiginificant variables at level 0.05.

Here, we can see that the significant factors affecting students' grades include, address, mother's level of education, studytime, ambition to further higher education, and going out.
Mother's education and higher education ambition seem to be the
most significatn factors. However, mother's education level is not curvillear(i.e does not have higher order polynomial(i.e "I(Medu^2)")


In accordance to the **_principle of parsimony_**, I decided to use variables with significance level of 0.05
Therefore, Final model:  grade_glm<-glm(G3~ address + Medu  + studytime + higher + goout,data=data_reg,family ="gaussian")

let's also see the if any observation has a leverage
and to further test the normality assumption while I also
explore the how the residuals differ from the fitted


```{r, message=FALSE, warning=FALSE}
par(mfrow=c(2,2))
plot(grade_glm, which = c(1,2,5))

```


from this, the constant variance of the error seem to be in line.
The distribution also appears to be normal from the Normal Q-Q plot although, slight divergence from the line at the lower level.
Also, no observation seem to have a clear leverage over others
aside a point lying quite farther from others. Nevertheless,
gaussian distribution can be safely utilised for modelling the
grde(G3) response variable.





##Mean Error and RMSE

**Create Functions to calculate Mean Error and Root Mean Square Error(RMSE)**
```{r, message=FALSE, warning=FALSE}
# function to calculate the mean absolute and RMSE
#function to calculate mean error
mean_error<- function(obs, pred){
  me<-mean(abs(obs-pred))
  return(me)
}

# Function that returns Root Mean Squared Error
rmse <- function(obs, pred){
  rmse<-sqrt(mean((obs-pred)^2))
  return(rmse)
}



```



###BOOSTRAPPING AND REGRESSION MODELLING(GLM, GAM & GBM).

**EVALUATION AND TESTING OF MODELS**

###MODELLING GRADE(G3).

This part is for the model validation. The data was divided into
70% training/calibration data and 30% testing/evaluation data. 
It was also boostrapped 10 times. The results of the models and
the response curves were thereafter presented.
dividing into 70:30
```{r, message=FALSE, warning=FALSE, include=FALSE}
#First copy, the original data into new dataframe, to avoid confusion
data_reg<-alco_data

{rep<-10
  for (i in 1:rep){
    #print the index to see the iteration
    print(i)
    
    #Creare a 70 sample(with replacement) from the original data
    rand<- sample(1:nrow(data_reg), size = 0.7*nrow(data_reg))
    
    #70% for the train/calibration data
    cal<- data_reg[rand,]
    
    #remaining 30 for the test/evaluation data
    eva<-data_reg[-rand,]
    
    ####GLM
    #perform a Genelralised Linear Model(GLM)
    grade_glm <- glm(G3~address + Medu  + studytime + higher + 
                       goout, data=cal, family = "poisson") 
    
    #predict into the test/evaluation data
    grade_glm_pred<- predict.glm(object = grade_glm, newdata = eva, type="response")
    
    #find the correlation between the train and test data.
    cor_glm_grade<-cor(grade_glm_pred, eva$G3, method = "spearman")
    
    
    #########
    #mean error and root mean square error
    #calculate the mean error
    error_grade_glm<- cbind.data.frame(grade_glm_pred, eva$G3)
    colnames(error_grade_glm) <- c("pred_glm_grade", "obs_grade")
    
    #Use the function created earlier to calulcate the mean error and RMSE.
    #Mean error
    grade_glm_me <- mean_error(error_grade_glm$obs_grade, error_grade_glm$pred_glm_grade)
    
    #RMSE
    grade_glm_rmse <- rmse(error_grade_glm$obs_grade, error_grade_glm$pred_glm_grade)
    
    #combine the dataframe of the mean error and RMSE
    me_rmse_grade_glm <- rbind.data.frame(grade_glm_me, grade_glm_rmse)
    
    #Change the column name to something more descriptive.
    colnames(me_rmse_grade_glm)<- c("grade_glm")
    
    
    
    
    #GAM
    grade_gam <- gam(G3~ s(Medu, k=3) + higher + address + s(studytime, k=3) + higher +
                       goout, data = cal, family = "gaussian")
    
    grade_gam_pred <- predict.gam(grade_gam, newdata = eva, type = "response")
    
    obs_pred_grade_gam<- cbind.data.frame(grade_gam_pred, eva$G3)
    colnames(obs_pred_grade_gam) <- c("pred_gam_grade", "obs_gam_grade")
    
    #you can just calculate the correlation straight away
    cor_gam_grade <- cor(grade_gam_pred, eva$G3, method = "spearman")
    
    
    #########
    #mean error and root mean square error
    error_grade_gam<- cbind.data.frame(grade_gam_pred, eva$G3)
    colnames(error_grade_gam) <- c("pred_gam_grade", "obs_grade")
    
    grade_gam_me <- mean_error(error_grade_gam$obs_grade, error_grade_gam$pred_gam_grade)
    grade_gam_rmse <- rmse(error_grade_gam$obs_grade, error_grade_gam$pred_gam_grade)
    
    me_rmse_grade_gam <- rbind.data.frame(grade_gam_me, grade_gam_rmse)
    colnames(me_rmse_grade_gam)<- c("grade_gam")
    
    
    
    
    ###################################################################3
    #using the normal gbm, package.
    #GBM
    grade_gbm1<-gbm(formula = G3~ sex + age+address+Medu+Fedu+
                      Pstatus+ traveltime+studytime+famsup+activities+higher+
                      internet+famrel+romantic+freetime+goout+ alc_use+ absences, data=cal,
                   distribution = "gaussian",n.trees = 1000, shrinkage = 0.001, interaction.depth = 6,
                   bag.fraction = 0.75)
    
    best.iter<-gbm.perf(grade_gbm1, plot.it = F, method = "OOB")
    grade_gbm1_pred<- predict.gbm(object = grade_gbm1, newdata = eva, best.iter,
                                 type="response")
    cor_gbm1_grade <- cor(grade_gbm1_pred, eva$G3, method = "spearman")
    
    
    
    #########
    #mean error and root mean square error
    error_grade_gbm1<- cbind.data.frame(grade_gbm1_pred, eva$G3)
    colnames(error_grade_gbm1) <- c("pred_gbm1_grade", "obs_grade")
    
    grade_gbm1_me <- mean_error(error_grade_gbm1$obs_grade, error_grade_gbm1$pred_gbm1_grade)
    grade_gbm1_rmse <- rmse(error_grade_gbm1$obs_grade, error_grade_gbm1$pred_gbm1_grade)
    
    me_rmse_grade_gbm1 <- rbind.data.frame(grade_gbm1_me, grade_gbm1_rmse)
    colnames(me_rmse_grade_gbm1)<- c("grade_gbm1")
    
    
    ###################################################
    #GBM(dismo package)
    
    grade_gbm2 <- gbm.step(data=cal, gbm.x =c("sex", "age","address","Medu"
                 ,"Fedu","Pstatus", "traveltime","studytime","famsup","activities","higher",
                 "internet","famrel","romantic","freetime","goout", "alc_use", "absences"), gbm.y = "G3",
                          bag.fraction=0.75, learning.rate = 0.001,
                          family="gaussian",n.trees=50, n.folds=10,
                          max.trees = 1000, tree.complexity = 6)
    
    #This also works but can be done directly as shown in the prediction after this
    #best.iter2<-gbm.perf(grade_gbm1, plot.it = T, method = "OOB")
    # grade_gbm2_pred<- predict.gbm(object = grade_gbm2, newdata = eva, best.iter2,
    #                              type="response")
    
    #
    #the prediction
    grade_gbm2_pred <- predict.gbm(grade_gbm2, newdata = eva, n.trees=grade_gbm2$n.trees, type = "response")
    #grade_pred_gbm2_ras <- predict(object=ras_stack,model=grade_gbm2, fun=predict,
    #                         n.trees=grade_gbm2$n.trees, type="response")
    
    #plot(grade_pred_gbm2_ras)
    cor_gbm2_grade <- cor(grade_gbm2_pred, eva$G3, method = "spearman")
    
    #########
    #mean error and root mean square error
    error_grade_gbm2<- cbind.data.frame(grade_gbm2_pred, eva$G3)
    colnames(error_grade_gbm2) <- c("pred_gbm2_grade", "obs_grade")
    
    grade_gbm2_me <- mean_error(error_grade_gbm2$obs_grade, error_grade_gbm2$pred_gbm2_grade)
    grade_gbm2_rmse <- rmse(error_grade_gbm2$obs_grade, error_grade_gbm2$pred_gbm2_grade)
    
    me_rmse_grade_gbm2 <- rbind.data.frame(grade_gbm2_me, grade_gbm2_rmse)
    colnames(me_rmse_grade_gbm2)<- c("grade_gbm2")
    
  } 
  #####All correlation
  all_cor_grade <- cbind.data.frame(cor_glm_grade,cor_gam_grade,
                                   cor_gbm1_grade, cor_gbm2_grade)
  colnames(all_cor_grade)<- c("grade_glm", "grade_gam", "grade_gbm1", "grade_gbm2")
  
  #####all error
  all_error_grade <- cbind.data.frame(me_rmse_grade_glm, me_rmse_grade_gam,
                                     me_rmse_grade_gbm1, me_rmse_grade_gbm2)
  rownames(all_error_grade)<- c("mean abs error", "RMSE")
  
}


```

###VALIDATION OF REGRESSION MODELS FOR GRADE(G3)

**let's see the correlation between the predicted and observed response variable**

```{r, message=FALSE, warning=FALSE}
#correlation between the predicted and observed grade(G3)
all_cor_grade
```

They all are low and not so much different.


**Below, we can see the errors of the various models.

```{r, message=FALSE, warning=FALSE}
#error of all the models
all_error_grade
```

There appears to be not much difference in the errors of all the models used.


###GAM and GBM for GRADE(G3) MODELLING

I chose GBM because it is able to handle multicollinearity and complex interactions, it can also show the response curves and relative importance of predictors.
GAM is also able to show response curves and their confidence intervals.

```{r, message=FALSE, include=FALSE, warning=FALSE}
#Using all the models to see the prediction
grade_gbm1<-gbm(formula = G3~ sex + age+address+Medu+Fedu+
          Pstatus+traveltime+studytime+famsup+activities+higher+           internet+famrel+romantic+freetime+goout+ alc_use+                   absences, data=data_reg,
                distribution = "gaussian",n.trees = 1000,                        shrinkage = 0.001, interaction.depth = 6,
                bag.fraction = 0.75)



```


**SUMMARY OF THE GBM FOR GRADE(G3)**
```{r, message=FALSE, warning=FALSE}
summary(grade_gbm1)
```

Here, we can see that higher education pursuit, mother's educaton level, alcohol use, and absences seem to be most important factors affecting students performances.
This is quite in line with the results I got from GLM and GAM, however, only mother's education level and higher_use seem to be the significant factors. The least important factors are also shown in the GBM's summary of relative importance.


###RESONSE CURVES(GAM  GBM,)
**now, let's see the response curves of this from GAM**

```{r, message=FALSE, warning=FALSE}
grade_gam <- gam(G3~ s(Medu, k=3) + higher + address +                         s(studytime, k=3) + higher + goout, data =                       data_reg,family = "gaussian")
plot(grade_gam, pages=1)
```

Here, from the smooth curve from GAM, we can see that mother's education level has a positive effect on student's performance. However, the confidence interval especially at the lower level. is quite large, which shows that there is a wide range of uncertainty. Perharps, more observations needed.
This will be explored further in the response curves from GBM below.




```{r, message=FALSE, warning=FALSE}
best.iter1<-gbm.perf(grade_gbm1, plot.it = F, method = "OOB")

par(mfrow=c(2,2))
plot.gbm(grade_gbm1, "Medu", best.iter1)
plot.gbm(grade_gbm1, "higher", best.iter1)
plot.gbm(grade_gbm1, "alc_use", best.iter1)
plot.gbm(grade_gbm1, "absences", best.iter1)
par(mfrow=c(1,1))

```

As we can see here, the higher the level of education of the mother, their kids seem to perform better. Also, studentsä with intention to pursue higher education seem to perform better.
Alcohol use and absences reduces performance and can result in dramatic reduction if it becomes chronic.


###**PREDICTED VS OBSERVED GRAGE(G3) FROM GBM.
```{r, message=FALSE, warning=FALSE}
plot(predict.gbm(grade_gbm1, data_reg, best.iter1), data_reg$G3, 
     main="Observed vs Predicted grade")
lines(lowess(predict.gbm(grade_gbm1, data_reg, best.iter1), data_reg$G3), col="red", lwd=3)
r_grade <-cor.test(predict.gbm(grade_gbm1, data_reg, best.iter1), data_reg$G3)
r2grade <- r_grade$estimate^2
r2grade
legend("topleft", paste("r^2=", round(r2grade,3)))

```

We can see from the scatterplots that the selected variables, account for only
32% factors affecting the students' grades.





##MODELLING ABSENCE(GLM, GAM, GBM)

###MODEL EVALUATION AND VALIDATION.

For GLM, the selection of predictors was done as earlier by using stepwise regression, anova(Chis sq) test and significance test from the model summary.

```{r, message=FALSE, warning=FALSE, include=FALSE}
abse_glm<-glm(absences ~sex + age+address+Medu+Fedu+
                Pstatus+                                             traveltime+studytime+famsup+activities+higher+
                internet+famrel+romantic+freetime+goout+                       high_use+G3
              ,data=data_reg,family ="poisson")
summary(abse_glm)
stepAIC(abse_glm)
anova(abse_glm, test="Chisq")

```


**Final model**

```{r, message=FALSE, warning=FALSE}
data_reg<- alco_data
abse_glm<-glm(absences ~ sex + age + Medu + 
                Pstatus +  studytime +  higher + 
                internet + goout + alc_use + G3
              ,data=data_reg,family ="poisson")


summary(abse_glm)

anova(abse_glm, test="Chisq")

```

The factors that might be causing absence of students are above.


###TRAINING AND TESTING OF MODELS FOR "ABSENCES"
```{r, message=FALSE, warning=FALSE, include=FALSE}

#########################################################################

# function to calculate the mean absolute and RMSE
#function to calculate mean error
mean_error<- function(obs, pred){
  me<-mean(abs(obs-pred))
  return(me)
}

# Function that returns Root Mean Squared Error
rmse <- function(obs, pred){
  rmse<-sqrt(mean((obs-pred)^2))
  return(rmse)
}

#dividing into 70:30
{rep<-10
for (i in 1:rep){
  print(i)
  
  #it's not  necessary to use the 1:nrow(data) below. it can be only nrow(data)
  rand<- sample(1:nrow(data_reg), size = 0.7*nrow(data_reg))
  cal<- data_reg[rand,]
  eva<-data_reg[-rand,]
  
  ###GLM
  abse_glm <- glm(absences~sex + age + Medu + 
                    Pstatus +  studytime +  higher + 
                    internet + goout + high_use + G3, data=cal, family = "poisson") 
  
  abse_glm_pred<- predict.glm(object = abse_glm, newdata = eva, type="response")
  
  cor_glm_abse<-cor(abse_glm_pred, eva$absences, method = "spearman")
  
  
  #########
  #mean error and root mean square error
  error_abse_glm<- cbind.data.frame(abse_glm_pred, eva$absences)
  colnames(error_abse_glm) <- c("pred_glm_abse", "obs_abse")
  
  abse_glm_me <- mean_error(error_abse_glm$obs_abse, error_abse_glm$pred_glm_abse)
  abse_glm_rmse <- rmse(error_abse_glm$obs_abse, error_abse_glm$pred_glm_abse)
  
  me_rmse_abse_glm <- rbind.data.frame(abse_glm_me, abse_glm_rmse)
  colnames(me_rmse_abse_glm)<- c("abse_glm")
  
  
  
  
  #GAM
  abse_gam <- gam(absences~ sex + s(age, k=3) +s(Medu, k=3) + 
                    Pstatus+  s(studytime, k=3) +  higher + 
                    internet + goout + s(alc_use, k=3) +  s(G3, k=3), data = cal, family = "poisson")
  abse_gam_pred <- predict.gam(abse_gam, newdata = eva, type = "response")
  
  obs_pred_abse_gam<- cbind.data.frame(abse_gam_pred, eva$absences)
  colnames(obs_pred_abse_gam) <- c("pred_gam_abse", "obs_gam_abse")
  #you can just calclate the correlation straight away
  cor_gam_abse <- cor(abse_gam_pred, eva$absences, method = "spearman")
  
  
  #########
  #mean error and root mean square error
  error_abse_gam<- cbind.data.frame(abse_gam_pred, eva$absences)
  colnames(error_abse_gam) <- c("pred_gam_abse", "obs_abse")
  
  abse_gam_me <- mean_error(error_abse_gam$obs_abse, error_abse_gam$pred_gam_abse)
  abse_gam_rmse <- rmse(error_abse_gam$obs_abse, error_abse_gam$pred_gam_abse)
  
  me_rmse_abse_gam <- rbind.data.frame(abse_gam_me, abse_gam_rmse)
  colnames(me_rmse_abse_gam)<- c("abse_gam")
  
  
  ###################################################################
  #using the normal gbm, package.
  #GBM
  abse_gbm1<-gbm(formula = absences~ sex                                     +age+address+Medu+Fedu+Pstatus+traveltime+
                   studytime+famsup+activities+higher+
                   internet+famrel+romantic+freetime+goout+                        alc_use, data=cal,
                 distribution = "poisson",n.trees = 1000,                         shrinkage = 0.001, interaction.depth = 6,
                 bag.fraction = 0.75)
  
  best.iter<-gbm.perf(abse_gbm1, plot.it = F, method = "OOB")
  abse_gbm1_pred<- predict.gbm(object = abse_gbm1, newdata = eva, best.iter, type="response")
  cor_gbm1_abse <- cor(abse_gbm1_pred, eva$absences, method = "spearman")
  
  
  
  #########
  #mean error and root mean square error
  error_abse_gbm1<- cbind.data.frame(abse_gbm1_pred, eva$absences)
  colnames(error_abse_gbm1) <- c("pred_gbm1_abse", "obs_abse")
  
  abse_gbm1_me <- mean_error(error_abse_gbm1$obs_abse, error_abse_gbm1$pred_gbm1_abse)
  abse_gbm1_rmse <- rmse(error_abse_gbm1$obs_abse, error_abse_gbm1$pred_gbm1_abse)
  
  me_rmse_abse_gbm1 <- rbind.data.frame(abse_gbm1_me, abse_gbm1_rmse)
  colnames(me_rmse_abse_gbm1)<- c("abse_gbm1")
  
  
  
  ###################################################
  #dismo package
  
  abse_gbm2 <- gbm.step(data=cal, gbm.x =c("sex", "age","address","Medu"
                                           ,"Fedu","Pstatus", "traveltime","studytime","famsup","activities","higher",
                                           "internet","famrel","romantic","freetime","goout", "alc_use"), gbm.y = "absences",
                        bag.fraction=0.75, learning.rate = 0.001,
                        family="poisson",n.trees=50, n.folds=10,
                        max.trees = 1000, tree.complexity = 6)
  
  #This also works but can be done directly as shown in the prediction after this
  #best.iter2<-gbm.perf(abse_gbm1, plot.it = T, method = "OOB")
  # abse_gbm2_pred<- predict.gbm(object = abse_gbm2, newdata = eva, best.iter2,
  #                              type="response")
  
  #
  #the prediction
  abse_gbm2_pred <- predict.gbm(abse_gbm2, newdata = eva, n.trees=abse_gbm2$n.trees, type = "response")
  #abse_pred_gbm2_ras <- predict(object=ras_stack,model=abse_gbm2, fun=predict,
  #                         n.trees=abse_gbm2$n.trees, type="response")
  
  #plot(abse_pred_gbm2_ras)
  cor_gbm2_abse <- cor(abse_gbm2_pred, eva$absences, method = "spearman")
  
  #########
  #mean error and root mean square error
  error_abse_gbm2<- cbind.data.frame(abse_gbm2_pred, eva$absences)
  colnames(error_abse_gbm2) <- c("pred_gbm2_abse", "obs_abse")
  
  abse_gbm2_me <- mean_error(error_abse_gbm2$obs_abse, error_abse_gbm2$pred_gbm2_abse)
  abse_gbm2_rmse <- rmse(error_abse_gbm2$obs_abse, error_abse_gbm2$pred_gbm2_abse)
  
  me_rmse_abse_gbm2 <- rbind.data.frame(abse_gbm2_me, abse_gbm2_rmse)
  colnames(me_rmse_abse_gbm2)<- c("abse_gbm2")
  
  
} 
#####All correlation
all_cor_abse <- cbind.data.frame(cor_glm_abse,cor_gam_abse,
                                 cor_gbm1_abse, cor_gbm2_abse)
colnames(all_cor_abse)<- c("abse_glm", "abse_gam", "abse_gbm1", "abse_gbm2")

#####all error
all_error_abse <- cbind.data.frame(me_rmse_abse_glm, me_rmse_abse_gam,
                                   me_rmse_abse_gbm1, me_rmse_abse_gbm2)
rownames(all_error_abse)<- c("mean abs error", "RMSE")

}


```



###correlation between the predicted and observed 
```{r, message=FALSE, warning=FALSE}
#correlation between the predicted and observed for all the models used.
all_cor_abse

#The mean correlation
all_cor_abse
```


###ERRORS OF THE MODELS IN PREDICTING "ABSENCES"
```{r, message=FALSE, warning=FALSE}
#let's see the errors in the prediction for "absence" response variable.
all_error_abse

```




###FITTING THE REGRESSION FOR "ABSENCES" WITH GBM
```{r, message=FALSE, warning=FALSE, include=FALSE}
#Using all the models to see the prediction
abse_gbm1<-gbm(formula = absences~ sex + age+address+Medu+Fedu+
                 Pstatus+traveltime+studytime+famsup+activities+                 higher+ internet+famrel+romantic+freetime+goout+                 alc_use, data=data_reg,
               distribution = "poisson",n.trees = 1000,                       shrinkage = 0.001, interaction.depth = 6,
               bag.fraction = 0.75)
#Summary of the model
summary(abse_gbm1)

```


I chose GBM because it is able to handle multicollinearity and complex interactions,
it can also show the response curves and relative importance of predictors.

Alcohol use, age, mother's education, parents' status, freetime and romantic relationship ostensibly, have the most effect on the cause of absences. Travel time and address seem to be the least



###RESPONSE CURVES OF "ABSENCES"
**GAM**
```{r, message=FALSE, warning=FALSE}
#now, let's see the response curves of this
#use all the dataset
abse_gam <- gam(absences~ sex + s(age, k=3) +s(Medu, k=3) + 
                  Pstatus+  s(studytime, k=3) +  higher + 
                  internet + goout + s(alc_use, k=3) +  s(G3,                  k=3), data = alco_data, family = "poisson")
plot(abse_gam, pages=1)

```



Teenage students are more likely to be absent although, there seems
not to be enough data for older students above 20, as the confidence interval'
seem high. Also. the likelihood of absences reduces with increase in the level
of education of the mother. Studytime also reduces this. While Alcohol increases
this. It is quite unsure how grade affects.


Furthermore, I will be expploring this further with GBM reponse curves.


**RESPONSE CURVES(GBM) FOR "ABSENCES"**
```{r, message=FALSE, warning=FALSE}
best.iter1<-gbm.perf(abse_gbm1, plot.it = F, method = "OOB")


par(mfrow=c(2,3))
plot.gbm(abse_gbm1, "alc_use", best.iter1)
plot.gbm(abse_gbm1, "age", best.iter1)
plot.gbm(abse_gbm1, "Medu", best.iter1)
plot.gbm(abse_gbm1, "Pstatus", best.iter1)
plot.gbm(abse_gbm1, "freetime", best.iter1)
plot.gbm(abse_gbm1, "romantic", best.iter1)
par(mfrow=c(1,1))

```


Alcohol use, age mother's education seem to increase the tendency to be absent
while freetime does the opposite. This is quite surprising as, I had expected the
exact opposite.
Also, students' with parents apart have more tendency to be absent that
those with their parents together. Likewise, students in romantic relationship
are more likely to be absent than those without.




###PREDICTED VS OBSERVED ABSENCES(COEFFICIENT OF DETERMINATION.) 
```{r, message=FALSE, warning=FALSE}
plot(predict.gbm(abse_gbm1, data_reg, best.iter1), data_reg$absences, 
     main="Observed vs Predicted absences")
lines(lowess(predict.gbm(abse_gbm1, data_reg, best.iter1), data_reg$absences), col="red", lwd=3)
r_abse <-cor.test(predict.gbm(abse_gbm1, data_reg, best.iter1), data_reg$absences)
r2abse <- r_abse$estimate^2
r2abse
legend("topleft", paste("r^2=", round(r2abse,3)))

```

We can see from the scatterplots that the selected variables, account for only 23%(the coefficient of determination) factors affecting the students' absences.





##MODELLING HIGH ALCOHOL CONSUMPTION.

Similar step was repeated here. However, in evaluating the models, I utilised **Area Under Curve(AUC), odds ratio and confusion/classification matrix**,  instead.


```{r, message=FALSE, warning=FALSE, include= FALSE}
#HIGH ALCOHOL

#Initial model for GLM
halc_glm<-glm(high_use ~sex + age+address+Medu+Fedu+
                Pstatus+ traveltime+studytime+famsup+activities+higher+
                internet+famrel+romantic+freetime+goout+ absences
              ,data=data_reg,family ="binomial")

summary(halc_glm)

stepAIC(halc_glm, direction = "both")

```

**Final model(GLM) for high alcohol use**

```{r, message=FALSE, warning=FALSE}
halc_glm<-glm(high_use ~sex + studytime + goout + absences
              ,data=data_reg,family ="binomial")

anova(halc_glm, test = "Chisq")
```



```{r, message=FALSE, warning=FALSE, include=FALSE}

############################################################
#####################################

#dividing into 70:30
{
  
  halc_auc_glm<-halc_auc_gam<-halc_auc_gbm1<-halc_auc_gbm2<-c()  
  rep<-10
  for (i in 1:rep){
    print(i)
    
    #it's not  necessary to use the 1:nrow(data) below. it can be only nrow(data)
    rand<- sample(1:nrow(data_reg), size = 0.7*nrow(data_reg))
    cal<- data_reg[rand,]
    eva<-data_reg[-rand,]
    
    ####
    #GLM
    halc_glm <- glm(high_use~  sex + studytime + goout +               absences, data=cal, family = "binomial") 
    
    halc_glm_pred<- predict.glm(object = halc_glm, newdata = eva, type="response")
    #check the AUC of the compared prediction and evaluation
    halc_auc_glm_p<-colAUC(halc_glm_pred, eva$high_use , plotROC=F)
    halc_auc_glm <- c(halc_auc_glm, halc_auc_glm_p[[1]])
    
    #GAM
    halc_gam<-gam(high_use~ sex+ s(studytime, k=3) + s(goout, k=3) + 
                    s(absences, k=3) , data = cal, family = "binomial")
    
    pred_halc_gam<-predict.gam(halc_gam, newdata = eva, type = "response")
    halc_auc_gam_p<-colAUC(pred_halc_gam, eva$high_use , plotROC=F)
    halc_auc_gam <- c(halc_auc_gam, halc_auc_gam_p[[1]])
    
    #GBM1
    halc_gbm1<-gbm(formula = high_use~ sex + age+address+Medu+Fedu+
                     Pstatus+ traveltime+studytime+famsup+activities+higher+
                     internet+famrel+romantic+freetime+goout+ absences, data=cal,
                   distribution = "bernoulli",n.trees = 2800, shrinkage = 0.001, interaction.depth = 6,
                   bag.fraction = 0.75)
    
    best.iter1<-gbm.perf(halc_gbm1, plot.it = F, method = "OOB")
    pred_halc_gbm1<-predict.gbm(halc_gbm1,newdata = eva, best.iter1, type = "response")
    
    halc_auc_gbm_p1<-colAUC(pred_halc_gbm1, eva$high_use , plotROC = F)
    halc_auc_gbm1<- c(halc_auc_gbm1, halc_auc_gbm_p1[[1]])
    
    #GBM2 dismo
    halc_gbm2<-gbm.step(data=cal, gbm.x =
    c("sex", "age","address","Medu","Fedu","Pstatus",
      "traveltime","studytime","famsup","activities","higher",
"internet","famrel","romantic","freetime","goout","absences"), gbm.y = "high_use",bag.fraction=0.75, learning.rate = 0.001,
family="bernoulli",n.trees=50, n.folds=10,max.trees = 3000, tree.complexity = 6)
    
    #prediction
    pred_halc_gbm2 <- predict.gbm(halc_gbm2, newdata = eva, n.trees=halc_gbm2$n.trees, type = "response")
    #The above can also be done usin the next two steps below.
    # best.iter2<-gbm.perf(halc_gbm2, plot.it = F, method = "OOB")
    # pred_halc_gbm2<-predict.gbm(halc_gbm2,newdata = eva, best.iter2, type = "response")
    # 
    
    #plotting ROC curve and getting the value
    halc_auc_gbm_p2<-colAUC(pred_halc_gbm2, eva$high_use , plotROC = F)
    #coombining t he value into a list
    halc_auc_gbm2<- c(halc_auc_gbm2, halc_auc_gbm_p2[[1]])
    
    
  } 
  compared_model_halc=cbind.data.frame(halc_auc_glm, halc_auc_gam, halc_auc_gbm1,halc_auc_gbm2)
}

```



###EVALUATION OF ALCOHOL CONSUMPTION'S MODEL.
```{r, message=FALSE, warning=FALSE}
# AUC values through the various iterations.
compared_model_halc


#The mean AUC values for the various models
mean_auc_halc<-colMeans(compared_model_halc)
#  attach(compared_model_halc)
#This was used temporarily to see if other models are significantly better.

# wilcox.test(halc_auc_gbm1, halc_auc_gam, paird=T)

#let's see the mean auc values for all the models
mean_auc_halc

```


**ROC(AUC) curves of the various models. **
```{r, message=FALSE, warning=FALSE}
#show the AUC curves from the last iteration on the test data
par(mfrow=c(2,2))
colAUC(halc_glm_pred, eva$high_use , plotROC = T)
colAUC(pred_halc_gam, eva$high_use , plotROC = T)
colAUC(pred_halc_gbm1, eva$high_use , plotROC = T)
colAUC(pred_halc_gbm2, eva$high_use , plotROC = T)
par(mfrow=c(1,1))


```


Here, I utilised Area Under Curve for evaluating my model. This
is because it prevents subjectiveness in selecting thresholds which can
significantly affect the predictive performance and commission and omission error.
Although, measures, such as prevalence have been recommended for dealing
with selection of threshold to conver into binary(e.g, True or False).
AUC readily takes care of this by considering all the possible thresholds
and evaluates the performance, accordingly. a value of >0.9 is an excellent model
while AUC values with range 0.7-0.8, 0.5-0.7, 0.5 are fairly good, poor and
very poor respectively.
Here, my AUC values for all the models thorugh my boosting and resampling
are about 0.7 for the three different models(GLm, GAM, GBM) which I applied.


##Model for alcohol use
###RESPONSE CURVES AND MODEL INTERPRETATIONS.
```{r, message=FALSE, warning=FALSE}

####################################################
#Model for alcohol use
#RESPONSE CURVES AND MODEL INTERPRETATIONS.
#Here, i decided to use the entire data for the analysis
#GAM
halc_gam<-gam(high_use~ sex+ s(studytime, k=3) + s(goout, k=3) + 
                s(absences, k=3) , data =data_reg, family = "binomial")
summary(halc_gam)
#plot the response curves from GAM
plot(halc_gam, pages=1)
```

From the above, we can see the response of high alcohol use to the various
predictors and also the confidence interval. There appears to be lesser tendency
of high alcohol use when study time increases. This is expected, as the student
has lesser time for social activities and parties.
on the other hand, expectedly, going out increases the tendency of high alcohol
use. In similar vein, absence from school increases the tendency too.
but as it can be seen from the plot, the confidence interval seems to reduce
with increase in number of absences, which might be an indication of
insufficient data from that region.


To get a deeper, insight, I will explore this further with **GBM**


```{r, message=FALSE, warning=FALSE}
#GBM
halc_gbm1<-gbm(formula = high_use~ sex + age+address+Medu+Fedu+
                 Pstatus+ traveltime+studytime+famsup+activities+higher+
                 internet+famrel+romantic+freetime+goout+                          absences, data=data_reg,
               distribution = "bernoulli",n.trees = 2800,                       shrinkage = 0.001, interaction.depth = 6,
               bag.fraction = 0.75)
summary(halc_gbm1)

```

From the relative importance, we can see that goout, absences, sex and family
relationship seem to have the highest effect on high alcohol use.
The least important factors can also be seen from the model summary.


Now, i'll show the response curve from GBM to see the effects
```{r, message=FALSE, warning=FALSE}
#Now, i'll show the response curve from GBM to see the effects

#plot(halc_gbm1)
best.iter1<-gbm.perf(halc_gbm1, plot.it = F, method = "OOB")

# pred_halc_gbm1<-predict.gbm(halc_gbm1,newdata = eva, best.iter1, type = "response")

par(mfrow=c(2,2))
plot.gbm(halc_gbm1, "goout", best.iter1)
plot.gbm(halc_gbm1, "absences", best.iter1)
plot.gbm(halc_gbm1, "sex", best.iter1)
plot.gbm(halc_gbm1, "famrel", best.iter1)
par(mfrow=c(1,1))
plot.gbm(halc_gbm1, "studytime", best.iter1)

```

As it can also be seen from the GM reponse curves, absences and going out,
increases the tendency of high alcohol use. Going out steeply affects when
it is more than average. Absences of slighty higher than 10 times can
even increase the tendency. Family relationship however, reduces the tendency.
and overall, male students seem to have relatively more high alcohol use than
their female counterpart. Also, as shown earlier, more study time appears
to reduce the tendency of high alcohol use.


###ODDS RATIO
```{r, message=FALSE, warning=FALSE}
#explore the odd's ratio
halc_glm_mod <- glm(high_use~  sex + studytime + goout + absences, data=data_reg, family = "binomial") 
odds_ra <- exp(coef(halc_glm_mod))
#odds_ra <- coef(high_use_mod2) %>% exp     #alternaive

# compute confidence intervals (conf_int)
conf_int <- exp(confint(halc_glm_mod)) 
#Conf_Int <- high_use_mod2 %>%  confint() %>% exp   #alternative

# print out the odds ratios with their confidence intervals
cbind(odds_ra, conf_int)


```

Here, we can also see that the odd's ratio of Male students, absences
and going out are more than 1, which indicates success: that they all increase
the tendency of high alcohol consumption by students. This is
not surprising. Absence from school and going out would normally be
expected to result in high alcohol use.
The confidence interval shows that it is highly likely that these
factors affect.
On the other hand, study time shows failure with high alcohol consumption
which is also not surprising, because when studying more, one
would expect that students have less time for going out and
getting drunk.
These results are all in accordance with the results of the
models used earlier.




###CONFUSION MATRIX
ERROR IN PREDICTD HIGH ALCOHOL USE.
Although, AUC is preferred, as it considers all possible threshold
and prevent subjectiveness, I will be exploring a confusion matrix too
and 0.5 will be made the threshold for TRUE or FALSE for high
alcohol use. More information on creating functions to calculate
these metrics in a more automated way can be found [here](http://rpubs.com/prcuny/161764)

```{r, message=FALSE, warning=FALSE}
#fit the model
# predict() the probability of high_use
probs<- predict(halc_glm_mod, type = "response")

#Copy the data again
alc<-data_reg

#add the predicted probabilities to ‘alc’
alc$prob_high_use <- probs
#alc <- mutate(alc, probability = probabilities)

#use the probabilities to make a prediction of high_use, setting 0.5 as threshold
alc$predict_high_use<- (alc$prob_high_use)>0.5
#alc <- mutate(alc, prediction = prob_high_use>0.5)

#see the first ten and last ten original classes, predicted probabilities, and class predictions
head(alc[,c("failures", "absences", "sex", "high_use", "prob_high_use", "predict_high_use")], n=10)

#tabulate the target variable versus the predictions
table(high_use = alc$high_use, prediction = alc$predict_high_use)

#Here, I derived the various classification matrix's metrics
#Accuracy: Overall, how often is the classifier correct?
#(TP+TN)/total
accuracy<- (252 + 49)/nrow(alc)
print(paste("The accuracy of the classification is", accuracy))

#Error rate
print(paste("The error rate/misclassification is", 1-accuracy))


#True Positive Rate:
#"Sensitivity" or "Recall": When actually yes, how often does it predict yes?
#(TP/actual yes)
print(paste("The True positive rate/Sensitivity is", 49/(65+49)))

#False Positive Rate: i.e When actually no, how often does it predict yes?
print(paste("The false positive rate is", 16/(16+252)))

#Specificity: When actually no, how often does it predict no?
#TN/actual no
#Alternatively: 1 minus False Positive Rate
print(paste("The Specificity is", 252/(252+16)))

#Precision: When it predicts yes, how often is it correct?
#TP/predicted yes
print(paste("The precision is", 49/(16+49)))

#Prevalence: How often does the yes condition actually occur in our sample?
#actual yes/total
print(paste("The prevalence is", (65+49)/nrow(alc)))

```


Accuracy of the prediction is about 0.79 which is fairly good.
Miclassification/error rate is about 0.21
By using threshold of 0.5, we can see that 252 FALSE high alcohol
use were Truly/rightly predicted and 16 falsely predicted.
There are also 49 high alcohol use that were Truly predicted
while 65, wrongly predicted. The prediction seems to be better
when predicting no high alcohol use(i.e Specifity) than when
predicting the true value(sensitivity. This can also be seen in the plot below.




**Plotting the error of prediction.**
```{r, message=FALSE, warning=FALSE}
# initialize a plot of 'high_use' versus 'probability' in 'alc'
g <- ggplot(alc, aes(x = prob_high_use, y = high_use, col= predict_high_use))

# define the geom as points and draw the plot
g + geom_point()


```


**ALTERNATIVE FOR CACLULATING THE ERROR**
```{r, message=FALSE, warning=FALSE}
#Proportion of errors
conf_mat<-table(high_use = alc$high_use, prediction = alc$predict_high_use)
conf_mat<-prop.table(conf_mat)
addmargins(conf_mat)

#mean error of the prediction
mean(abs(alc$high_use-alc$prob_high_use)>0.5)

#The below is an alternative way by firstly defining the function to calculate the mean error.
# define a loss function (mean prediction error)
loss_func <- function(class, prob) {
  n_wrong <- abs(class - prob) > 0.5
  mean(n_wrong)
}

# call loss_func to compute the average number of wrong predictions in the (training) data
loss_func(class = alc$high_use, prob = alc$prob_high_use)

```


**K-fold cross-validation**

```{r, message=FALSE, warning=FALSE}

#K-fold cross-validation
library(boot)
cv <- cv.glm(data = alc, cost = loss_func, glmfit = halc_glm_mod, K = 10)

# average number of wrong predictions in the cross validation
cv$delta[1]
```


Here I used an arbitrary threshold of 0.5 but it has been
suggested that prevalence can be used in selection of threshold.
you can see more [here](http://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/)
about terminologies related to confusion/classification
matrix.


##LINEAR DISCRIMINANT ANALYSIS(LDA).

```{r, message=FALSE, warning=FALSE}
###############################################################
#LINEAR DISCRIMINANT ANALYSIS.


#copy data again
data_copy2<- alco_data

#filter the numeric variables.
data_num<-Filter(is.numeric, data_copy2)

#Scale the numeric variables
data_num_sca<- data.frame( scale(data_num))

#get the categorised grades created earlier into the vector
G3_cat<-data_mca_fac2[,c("G3")]

#add to to the scaled data frame
data_num_G3<- cbind(data_num_sca, G3_cat)

#summary
summary(data_num_G3)
```






**DIVIDE DATA INTO TEST AND TRAIN DATA FOR VALIDATION.**
```{r, message=FALSE, warning=FALSE}
#now, divide into 80:20 train:test data
samp <- sample(1:nrow(data_num_G3), size = nrow(data_num_G3)*0.8)
train <- data_num_G3[samp,]
test <- data_num_G3[-samp,]


#Dimension.
dim(train)

#remove the grades columns
train$G1<-train$G2<-train$G3<-NULL

#check the column names now
colnames(train)
```


**fitting the categorised grade to all other variables**
```{r, message=FALSE, warning=FALSE}
#fit the categorised grade to all other variables
lda.fit <- lda(G3_cat~., data = train)
#lda.fit

#create arrow function
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

#convert the categorised grade into numeric for the LDA plot
train$G3_cat <- as.numeric(train$G3_cat)

#plot
plot(lda.fit, dimen = 2, col = train$G3, pch= train$G3)
lda.arrows(lda.fit, myscale = 2)
```

failures in the past is contributing most to poor grades.
Absences also appear to be. Mother's education level seems to be a factor contributing to students' success.


**LDA CLASSIFICATION ACCURACY**
```{r, message=FALSE, warning=FALSE}
#see the class prediction accuracy
G3_cat<-test$G3_cat
test<-dplyr::select(test, -G3_cat)
lda.pred<-predict(lda.fit, newdata = test)
table(correct = G3_cat, predicted = lda.pred$class)
```

The table above shows the right classification and misclassification.


**Distance between variables.**
```{r, message=FALSE, warning=FALSE}
#Now, see the distance between variables.
dist_sca<-dist(data_num_sca)
summary(dist_sca)
```

Next, cluster the data better using k-means clustering and refit the LDA using the clustered data.

First, determine the optimal number of clusters using the elbow
method of **TWCSS**.

**Distance between variables.**
```{r, message=FALSE, warning=FALSE}
set.seed(123) #This is used to make the values constant when re-run
# determine the number of clusters
k_max <- 20

# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(data_num_sca, k)$tot.withinss})

# visualize the results
qplot(x = 1:k_max, y = twcss, geom = c('point', 'line'))

############################################################
#some other methods of exploring optimal number of classes
# library(cluster)
# library(factoextra)
# # reVisualize k-means clusters
# km.res <- kmeans(data_num_sca, 3, nstart = 25)
# fviz_cluster(km.res, data = data_num_sca, geom = "point",
#              stand = FALSE, frame.type = "norm")
# 
# #install.packages("factoextra")
# require(cluster)
# fviz_nbclust(data_num_sca, kmeans, method = "silhouette")
#############################################################
```


**Further, use the NbClust to confirm. There are quite many others too. Two others are in the previous code chunk where the packages "cluster" and "factoextra" can be used. See more [here](http://www.sthda.com/english/wiki/print.php?id=239#required-packages) for other cluster methods of checking the optimal number of classes.


```{r, message=FALSE, warning=FALSE}
#install.packages("cluster")
library(NbClust)
nb <- NbClust(data_num_sca, diss=NULL, distance = "euclidean", 
              min.nc=2, max.nc=5, method = "kmeans", 
              index = "all", alphaBeale = 0.1)

#Based on these, I will make my number of clusters, 3

# k-means clustering
#use the euclidean distance calculated earlier.
km <-kmeans(dist_sca, centers = 3)

#perform LDA on the clustered data
lda.fit_clus<- lda(km$cluster~., data=data_num_sca)


plot(lda.fit_clus, dimen = 2, col = km$cluster, pch= km$cluster)
lda.arrows(lda.fit_clus, myscale = 2)
```
Seems past failure and alcohol use also causes poor performance


```{r, message=FALSE, warning=FALSE}
#copydata again
data_copy2<-alco_data
data_num2<-Filter(is.numeric, data_copy2)
data_LDA1_sca<- data.frame( scale(data_num2))
summary(data_LDA1_sca)

#removwe the alcohol columns
data_LDA1_sca$alc_use<-data_LDA1_sca$Walc<-data_LDA1_sca$Dalc<-NULL

#see colmn names
colnames(data_LDA1_sca)

#Get the alcohol use into a cector
alc_use<-alco_data[,c("alc_use")]

#combine with the scaled data
data_LDA2<- cbind(data_LDA1_sca, alc_use)


#summary(data_LDA2)

lda.fit2 <- lda(alc_use~., data = data_LDA2)
#lda.fit


#first, determine the optimal number of clusters using twcss.
set.seed(123)
# determine the number of clusters
k_max <- 10
alc_sca<-as.data.frame(scale(data_LDA2))
# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(alc_sca, k)$tot.withinss})

# visualize the results
qplot(x = 1:k_max, y = twcss, geom = 'line')

# k-means clustering
dist_alc<- dist(alc_sca)
km <-kmeans(dist_sca, centers = 4)


#perform LDA on the clustered data
lda.fit_clus<- lda(km$cluster~., data=alc_sca)
lda.fit_clus

#plot
plot(lda.fit_clus, dimen = 2, col = km$cluster, pch= km$cluster)
lda.arrows(lda.fit_clus, myscale = 2)
```


Here, we can see that going out, free time and absences tend
tend to cause high alcohol use.
On the other hand, study tume, family relationship tend to reduce it.


**3D VISUALISATION OF THE CLUSTERS**
```{r, message=FALSE, warning=FALSE}
model_predictors <- dplyr::select(data_LDA2, -alc_use)

# matrix multiplication
matrix_product <- as.matrix(model_predictors) %*% lda.fit$scaling
matrix_product <- as.data.frame(matrix_product)

#Next, install and access the plotly package. Create a 3D plot (Cool!) of the columns of the matrix product by typing the code below.
library(plotly)
#using the plotly package for 3D plotting of the matrix products’ columns.

plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers')

plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers', color=data_LDA2$alc_use)


```


#CONCLUSION

Overall, the various analyses have shown the variation amongst the students. Female students seem to get more family and school supports than their male counterpart and are also able to attend paid extra classes more than the latter.

Generally, male students consume more alcohol highly and high alcohol consumption has been shown to have a negative effect on grade. Furthermore, going out has been linked to high alcohol consumption which is more common with students that are frequently absent.

On a good note, Mother's level of education plays a very vital role in students perfomance. In similar manner, ambition for higher education was found to be a considerable factor in the academic performances of the students and are more common amongst female students than male students.

On this note, I will be rejecting all my null hypotheses, as high alcohol consumption has a significant effect on students' performances. Students' grades are also affected by absence.

It is also noteworthy that more samples should be collected and also more variables, as most of the variables explained less than 50% of all my target/response variables :  grades, high alcohol consumption and absences.